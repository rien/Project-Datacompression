\documentclass[a4paper]{article}
\author{Rien Maertens}
\title{Verslag project DA3}
\date{11 mei 2016}
\usepackage[dutch]{babel}
\usepackage[utf8]{inputenc}
\usepackage{pgfplots}
\usepackage{float}
\usepackage{hyperref}
\pgfplotsset{compat=newest}
\setlength{\parindent}{1em}
\setlength{\parskip}{1em}
\begin{document}
\maketitle

\part{Standaard algoritme}

\section{Keuze van het algoritme}

De keuze van het algoritme is vooral gebaseerd op redeneringswerk. We vatten de eigenschappen van de mogelijke compressie-algoritmen samen:
\begin{description}
    \item [LZW] Compressiealgoritme waarin een woordenboek wordt bijgehouden die een codewoord van variabele lengte mapt op één of meerdere karakters. Dit algoritme is vooral efficient waneer vaak dezelfde (lange) reeks karakters in de data voorkomt. Omdat voor deze volledige reeks maar één codewoord nodig is.
    \item [LZ77] Bij dit compressie-algoritme wordt er met een sliding window gewerkt. Hier krjgen we dus ook efficiënte compressie wanneer langere reeksen karakters vaak voorkomen.
    \item [Huffman] Deze encodering bepaalt het aantal bits van een codewoord aan de hand van het aantal voorkomens in de gegeven data. Karakters die vaak voorkomen krijgen een kortere code, waardoor een goeie compressie wordt bereikt wanneer een klein aantal karakters vaak voorkomen.
    \item [Burrows-Wheeler] Dit algoritme maakt een tekst niet kleiner. Het is een transformatie die het gemakkelijker maakt om bestanden beter te comprimeren. Na deze transformatie wordt vaak move-to-front toegepast en vervolgens huffman encoding. Dit heeft enkel echter een invloed wanneer vaak dezelfde opeenvolging van karakters in de data zitten.
\end{description}

Huffman encoding leek mij de beste manier om de gegeven data te comprimeren. De gegeven data heeft maar een alfabet van dertien karakters: de tien cijfers, de komma, en de twee vierkante haakjes. Deze worden door huffman omgezet naar codewoorden van drie tot vier karakters. Hierdoor verwachten we minstens een halvering van de originele bestandsgrootte.

Ik heb niet gekozen voor de LZW of LZ77 omdat deze algoritmes langere codewoorden nodig hebben voor opeenvolgende karakters zonder structuur (zoals de minst significate cijfers van een groot getal). Terwijl dat Huffman voor ieder karakter in ons alfabet een kortere code gebruikt.

Naast Huffman heb ik ook Burrows-Wheeler in combinatie met move-to-front geïmplementeerd. Maar dit bleek achteraf geen goede keuze te zijn. De compressiewinst woog niet op tegen de extra rekentijd die nodig was om de transformatie te verwezenlijken. Daarnaast waren er ook wat problemen met het implementeren van de decompressie waardoor ik besloten heb de Burrows-Wheeler implementatie niet af te werken. De code en de tests zijn nog altijd terug te vinden in het project.

\section{Implementatie}

Bij mijn compressieprogramma heb ik een module die zich enkel bezighoud met het bitgewijs lezen en schrijven van data: \texttt{bitcode.h} en \texttt{bitcode.c}. Deze maken het mogelijk om één enkele bit of byte te lezen of te schrijven en om een bitcode te concateneren aan de ander. Om dit zo efficient mogelijk te maken worden er zo weinig mogelijk bitoperaties gebruikt. Bijvoorbeeld: twee keer een OR-operatie om een niet-gealigneerde byte te schrijven in plaats van acht OR-operaties met de individuele bits. De gealloceerde array van data groeit ook exponentieel (zoals een arraylist zou doen) om toevoegen in constante tijd te laten doorgaan.

\subsection{Huffman}

Mijn implementatie van het Huffman-algoritme volgt grotendeels dat die in de cursus bescheven staat. Dit zijn nog enkele implementatiedetails die het vermelden waard zijn:

De prioriteitswachtlijn die wordt gebruikt in mijn Huffman-implementatie is te vinden in \texttt{priorityqueue.c} en \texttt{priorityqueue.h}. De implementatie daarvan is redelijk naief: de items worden van groot naar klein gesorteerd, waardoor het kleinste element verwijderen in constante tijd gebeurt. Een nieuw element invoegen gebeurt door het item op het einde van de lijst toe te voegen en de hele lijst te sorteren. Aangezien de lijst een maximale grote heeft van 256 elementen (één per byte) en deze priorityqueue enkel wordt gebruikt bij het opbouwen van de Huffman-boom loont het niet de moeite om dit te optimaliseren.

De Huffman-boom opslaan is misschien ook iets wat meer uitleg vraagt. Ik schrijf de boom als volgt uit\footnote{Het idee van dit formaat komt van het volgende StackOverflow-antwoord: \url{http://stackoverflow.com/a/759766/4424838}}: we overlopen de boom in pre-orde en bij iedere top schrijven we uit:
\begin{itemize}
    \item De bit \texttt{0} wanneer we een inwendige top hebben.
    \item De bit \texttt{1} wanneer we een blad hebben, gevolgd door het karakter dat door die top wordt voorgesteld.
\end{itemize}
Op die manier kan er op een recursieve manier door de boom gelopen worden om ze op te slaan en kan ze ook recursief ingelezen worden, terwijl het codewoord voor ieder karakter wordt ingesteld (met de methodes \texttt{huffman\_build\_dictionary()} en \texttt{huffman\_reconstruct\_tree()} in \texttt{huffman.c}).

% TODO: meer  ontwerpsbeslissingen

\subsection{Burrows-Wheeler}

Na het implementeren van Burrows-Wheeler was het snel duidelijk dat dit algoritme niet voldeed aan mijn verwachtingen: de compressiewinst was te klein en het snelheidsverlies te significant. Dit is ook logisch, de Burrows-Wheeler transformatie is pas efficient wanneer de tekst vaak voorkomende opeenvolgingen van karakters heeft. Wat niet het geval is in onze testdata. Omdat ik besloten heb de Burrows-Wheeler implementatie niet af te werken heb ik hier helaas geen testresultaten van.

Er zijn misschien wel enkele implementatiedetails die het vermelden waard zijn:
\begin{itemize}
    \item Bij de encodering wordt er gebruikt gemaakt van circulaire strings (\texttt{circular\_string.c}) die een pointer bevatten naar de originele string en een offset die hun rotatie aangeeft. Dit maakt het sorteren en roteren efficiënter.
    \item Het sorteren van de strings gebeurt momenteel met \texttt{qsort}, wat hoogstwaarschijnlijk de grootste bottleneck was. Omdat er gebruik wordt gemaakt van functiepointers is het moeilijker om daar te optimaliseren. Handmatig het sorteeralgoritme schrijven zou waarschijnlijk een significate performantiewinst als resultaat hebben.
    \item De index van het originele eerste karakter in de string wordt niet meegecodeerd met huffman, maar op voorhand uitgeschreven.
    \item Bij het decoderen wordt er gewerkt met pointers naar karakters die worden gesorteerd. Aan de hand van die pointers en het laastst gedecodeerde karakter wordt de positie van het volgende karakter achterhaald.
\end{itemize}

% TODO: resultaten



\part{Specifiek algortime}

%TODO: bespreken algoritme
%TODO: waarom?

\part{Vergelijking algoritmes}

%TODO: best en worst-case

\part{Versturen over internet}

%TODO: bespreken

\end{document}
